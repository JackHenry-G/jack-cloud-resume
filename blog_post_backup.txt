My Cloud Resume Challenge
#
aws
#
webdev
#
serverless
#
devops
A little background on me and the project
I am a software developer apprentice with about 1-2 years experience in the role but have been getting to grips with code since secondary school (around 14). So I'm more than comfortable with writing business logic code and have some experience with DevOps (mostly docker). Now whilst I know code, I had never deployed a serverless website complete with automated CI/CD and Infrastructure as Code (IaC) before so it was daunting. But the challenge was a fun experience because of the great resources out there and I would recommend it for beginners! It may take some time but it was super rewarding :)

Flashback to January (now end of March) I had just completed the AWS Cloud Practitioner Certification when I was looking for some projects to improve my front-end skills when it dawned on me that whilst I had learned a lot with the certification it was more theory than practical application. That was when I found the AWS Cloud Resume Challenge by Forrest Brazeal - which would allow me to prove my skills and actually use the cloud rather than just know about it. I'd recommend reading the challenge description but it demonstrates skills in:

Front-end development - HTML, CSS, JavaScript
Back-end development - writing Lambda functions to create a visitor counter on your site, tracked with the DynamoDB service.
DevOps - implement CI/CD tools like GitHub actions to automate build, test and deployment
Infrastructure as Code - use tools like AWS SAM or Terraform to automate the creation and management of your cloud infrastructure
Networking and Cloud management - learn about how to configure the routing of your webpage to deliver content (Amazon CloudFront, Route 53, Certificate Manager, DNS etc., IAM management, etc.).
I worked through the steps in the order shown on the description, which in my opinion was good for learning purposes. However, in this blog I'll split it into different sections - front-end, creating the visitor counter, testing, source control and infrastructure as code. Whilst you could start with IaC first and be done with the project, I actually found playing around with the AWS Console the most beneficial and I would treat IaC as a different learning experience.

ADVICE
I REALLY recommend using CloudWatch logs to troubleshoot any issues your having with your Lambda which I didn't seem to see mentioned in other resources. You can do this by going to your 'monitor' section of your lambda function and clicking 'View CloudWatch Logs'. Combined with Postman application this is a much easier way of testing your functions.

Front-end - creating the website
This was the most simple section of the project, made difficult only by having decision paralysis on what template to choose - I ultimately landed on this Jackson template by Aigars Silkalns which I customized to my needs. I then created a S3 bucket to house my files, secured this with HTTPS using CloudFront which sits in front of S3 and delivers content to users via caches around the world.

I then purchased a domain name (jackgoggin.com) in Route53 and. retrieved appropriate certificates from Amazon Certificate Manger. This would route traffic from jackgoggin.com to my CloudFront with my S3 bucket content.

Image description

Useful resources:

Hosting a static website with S3
Static website with CloudFront
Back-end - creating the visitor counter
This was also quite a simple task once you know the appropriate services to use and architecture which will look like the image below.

Image description

The steps included:

Creating a DynamoDB Table (which are schema-less and very simple for this solution) with a count value
Creating a role with IAM that allows access to DynamoDB
Creating a lambda function, with the previous role applied, that when called with increment the visitor counter value and return to the web server
Create an API Gateway with a method that calls this lambda function when hit with a GET request and returns the JSON response
Enable CORS to allow relaxation of rules to facilitate our requests
Create a JavaScript function that will send a GET request to the appropriate API pathway and then update our page with the JSON response
Useful resources:

DynamoDB lambda functions with Python
Boto3 library
Testing of Lambda functions
This is where I started to panic a bit as I had no real understanding of where to start testing a serverless function. You can get real in depth by using integration and/or end-to-end testing, however, I decided to begin with Unit Testing which turned out to be not a problem at all.

At a high-level you can unit test you function by simply mocking the DynamoDB database with the Python library Moto. Which means you simply create a dummy database within your code and feed this to your Lambda function, which will use that as a database and feed you back an expected JSON response that you can test against. Easy!

Source control and CI/CD - managing changes to your code and automation of your testing with GitHub Actions
The challenge recommends that you create a separate Github repository for your front-end and back-end code, which I agreed with before the introduction of IaC where I then developed the whole project into one respos as that felt easier to manage.

The idea here with your source control is that when you make changes to your index.html, python code or SAM templating and upload that to Github, your changes should also be integrated into your AWS resources. For example, your S3 should be updated with your new index.html, your Lambda should be updated with your new Python and your whole CloudFormation stack should be updated with any changes to your SAM template. This is where GitHub Actions comes into play.

GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production. - GitHub docs

To implement this we simply setup a directory in our repos '.github/workflows' and create a INSERT_NAME.yml file that will define a series of jobs for GitHub to run when a push is made to the respoitory.

Before doing this, it is required that you setup a role in IAM with access keys that allow GitHub the appropriate permissions to start editing our AWS resources!

Within this workflow YAML file we are simply going to setup this order of actions:

Test our code
If step 1 was successful, build and manage our infrastructure (still need to implement the SAM file for this step). E.g. upload any infrastructure changes or changes to our lambda function.
Sync our front-end files with the S3 bucket. E.g. upload an changes to our website. (Also make sure to invalidate the cache of S3 to ensure any changes are properly uploaded.)
Useful resources:

GitHub Python testing
Open Up The Cloud series
Infrastructure as Code
GIFImage description

This is where things started to get complicated as documentation and resources started to get more complex. This wasn't easy as a newbie to serverless resources and with little experience in Infrastructure as Code but I got there in the end.

Firstly, IaC allows you capture and deploy all your infrastructure in a codified manner. So instead of clicking around on the AWS console, you define all you need in one file and simply execute one command to spin it all up or update it. This obviously improves efficiency but also consistency, agility, reusability, tracking and security.

To facilitate this you can either Terraform which a widely used service but I stuck with AWS SAM which is a slightly simpler version designed specifically for this use case of AWS. To use this you will need AWS CLI installed and Docker is another helpful tool - I recommend following Chris' blog linked in the resources below. Then all you need to do is create a 'template.yaml' file in the base of your code and start defining jobs in that file and then execute this with 'sam build --use-container && sam deploy --guided' (once you have setup SAM with AWS CLI). Your SAM file should define all the things we have already discussed like your S3 bucket, bucket policy, CloudFront distribution, Route 53 and DNS, Certificates, DynamoDB Table, Lambda Function and API Gateway.

Although this was the hardest part, it was also very rewarding and spinning up my whole infrastructure at the click of a button was a relief to pushing through error message after error message.

Useful resources:

Getting started with SAM (AWS Docs)
Stacy Stipes' blog
Chris' series of blogs about the cloud (one on Lambda and one on DynamoDB)
Summary
In hindsight, the project was tough and I never felt like quitting because I was always making progress and learning something new. It was a great experience and I learned A LOT and now I have something to show for all my effort. A refreshing experience compared to coding courses, creating projects is the best way for learning tech.

Final product.